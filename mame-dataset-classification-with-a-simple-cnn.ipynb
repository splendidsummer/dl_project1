{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "version": "3.6.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction\n",
    "In this notebook we present a simple CNN architecture to classify the MAMe dataset.\n",
    "With 3 Convolutional Layers and two fully connected layers we can achieve a classification accuracy of around ~~75%~~ 65%."
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Loading the dataset\n",
    "The function ```load_mame``` reads the ```Mame_dataset.csv``` table and returns, for the train, validation and test subsets, a Pandas DataFrame containing the image filenames and their corresponding class (if ```dataframe=True```) or an array of filenames and a list of the corresponding classes (if ```dataframe=False```).\n",
    "\n",
    "The DataFrame will be useful to use with Keras' ```ImageDataGenerator.flow_from_dataframe``` method."
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_mame(dataframe=False):\n",
    "    \"\"\" Load MAMe dataset data\n",
    "    Args:\n",
    "      dataframe (bool): whether to return a dataframe or an array of \n",
    "                        filenames and a list of labels\n",
    "      \n",
    "    Returns:\n",
    "      (x_train, y_train), (x_val, y_val), (x_test, y_test) if dataframe=False\n",
    "      or\n",
    "      df_train, df_val, df_test if dataframe=True\n",
    "    \"\"\"\n",
    "    INPUT_PATH = '/kaggle/input/'\n",
    "\n",
    "    # Load dataset table\n",
    "    dataset = pd.read_csv(os.path.join(INPUT_PATH, 'mame-dataset', 'MAMe_dataset.csv'))\n",
    "    \n",
    "    # Subset divisions\n",
    "    x_train_files = dataset.loc[dataset['Subset'] == 'train']['Image file'].tolist()\n",
    "    y_train_class = dataset.loc[dataset['Subset'] == 'train']['Medium'].tolist()\n",
    "\n",
    "    x_val_files = dataset.loc[dataset['Subset'] == 'val']['Image file'].tolist()\n",
    "    y_val_class = dataset.loc[dataset['Subset'] == 'val']['Medium'].tolist()\n",
    "\n",
    "    x_test_files = dataset.loc[dataset['Subset'] == 'test']['Image file'].tolist()\n",
    "    y_test_class = dataset.loc[dataset['Subset'] == 'test']['Medium'].tolist()\n",
    "\n",
    "    if dataframe:\n",
    "        train = pd.DataFrame({'filename': x_train_files, 'class': y_train_class})\n",
    "        val = pd.DataFrame({'filename': x_val_files, 'class': y_val_class})\n",
    "        test = pd.DataFrame({'filename': x_test_files, 'class': y_test_class})\n",
    "        \n",
    "        # Set full path\n",
    "        train['filename'] = train['filename'].transform(lambda x: INPUT_PATH + 'mame-dataset' + os.sep + 'data' + os.sep + x)\n",
    "        val['filename'] = val['filename'].transform(lambda x: INPUT_PATH + 'mame-dataset' + os.sep + 'data' + os.sep + x)\n",
    "        test['filename'] = test['filename'].transform(lambda x: INPUT_PATH + 'mame-dataset' + os.sep + 'data' + os.sep + x)\n",
    "        \n",
    "        return train, val, test\n",
    "    \n",
    "    else:\n",
    "        # Return list of filenames\n",
    "        x_train = [os.path.join(INPUT_PATH, 'mame-dataset', 'data', img_name) for img_name in x_train_files]\n",
    "        x_val = [os.path.join(INPUT_PATH, 'mame-dataset', 'data', img_name) for img_name in x_val_files]\n",
    "        x_test = [os.path.join(INPUT_PATH, 'mame-dataset', 'data', img_name) for img_name in x_test_files]\n",
    "\n",
    "        return (np.array(x_train), np.array(y_train_class)), (np.array(x_val), \n",
    "              np.array(y_val_class)), (np.array(x_test), np.array(y_test_class))\n",
    "    \n",
    "\n",
    "df_train, df_val, df_test = load_mame(dataframe=True)\n",
    "print(df_train.head())"
   ],
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plot examples\n",
    "Let's visualize an example of each class:"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from keras.preprocessing.image import load_img\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_images(img_df):\n",
    "    \"\"\" Show images\n",
    "    Plot a random sample of images for each of the class labels.\n",
    "    \n",
    "    Args:\n",
    "      img_df (DataFrame): DataFrame with a column 'filename' with image filenames and \n",
    "                          a colum 'class' with classification labels\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    i = 1\n",
    "    \n",
    "    classes = img_df['class'].unique().tolist()\n",
    "    for c in classes:\n",
    "        # Get a random sample of an instance with class c\n",
    "        filename = img_df[img_df['class']==c].sample(1)['filename'].values[0]\n",
    "        \n",
    "        # Plot image\n",
    "        plt.subplot(6, 5, i)\n",
    "        plt.imshow(load_img(filename))\n",
    "        plt.title(c, fontsize=16)\n",
    "        plt.axis('off')\n",
    "        i += 1\n",
    "                  \n",
    "    plt.show()\n",
    "                                                                                     \n",
    "show_images(df_train)"
   ],
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CNN\n",
    "To classify this dataset will build a simle deep learning model based on convolutional layers. This is be done using TensorFlow and Keras."
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import time\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import np_utils\n",
    "\n",
    "\n",
    "print('Using Keras version', keras.__version__)\n",
    "print('Using TensorFlow version', tf.__version__)\n",
    "\n",
    "# Initialize some variables\n",
    "img_height, img_width = 256, 256\n",
    "n_channels = 3\n",
    "input_shape = (img_height, img_width, n_channels)\n",
    "batch_size = 128\n",
    "\n",
    "#Â Load dataset\n",
    "df_train, df_val, df_test = load_mame(dataframe=True)\n",
    "num_classes = len(df_train['class'].unique())\n",
    "\n",
    "# Model architecture\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(5, 5), activation='relu', kernel_initializer='he_normal', padding='same', input_shape=input_shape, data_format='channels_last'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu', kernel_initializer='he_normal',padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(num_classes, activation=(tf.nn.softmax)))\n",
    "\n",
    "# Print model summary\n",
    "print(model.summary())"
   ],
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data augmentation and generators\n",
    "We use data augmentation on the train set and create generators for the train, validation and test set."
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Initiate the train and test generators with data Augumentation \n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1.0/255.0,\n",
    "        rotation_range = 30,\n",
    "        zoom_range = 0.2,\n",
    "        width_shift_range = 0.2,\n",
    "        height_shift_range = 0.2,\n",
    "        shear_range = 0.2,\n",
    "        horizontal_flip = True,\n",
    "        fill_mode = \"nearest\")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "        df_train,\n",
    "        target_size = (img_height, img_width),\n",
    "        batch_size = batch_size, \n",
    "        class_mode = \"categorical\",\n",
    "        validate_filenames=False)\n",
    "\n",
    "validation_generator = test_datagen.flow_from_dataframe(\n",
    "        df_val,\n",
    "        target_size = (img_height, img_width),\n",
    "        batch_size = batch_size,\n",
    "        shuffle = False, \n",
    "        class_mode = \"categorical\",\n",
    "        validate_filenames=False)\n",
    "\n",
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "        df_test,\n",
    "        target_size = (img_height, img_width),\n",
    "        batch_size = 1,\n",
    "        shuffle = False, \n",
    "        class_mode = \"categorical\",\n",
    "        validate_filenames=False)\n",
    "\n"
   ],
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train the model\n",
    "We compile the model using the Adam optimizer and then fit the train generator."
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=0.001, epsilon=0.1, amsgrad=True), metrics=[\"accuracy\"])\n",
    "\n",
    "# Use early stopping\n",
    "early = EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=10, verbose=1, mode='auto', restore_best_weights=True)\n",
    "\n",
    "# Train the model \n",
    "t0 = time.time()\n",
    "epochs = 100\n",
    "\n",
    "t0 = time.time()\n",
    "STEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size\n",
    "STEP_SIZE_VAL=validation_generator.n//validation_generator.batch_size\n",
    "    \n",
    "history = model.fit(x=train_generator,\n",
    "                    steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=STEP_SIZE_VAL,\n",
    "                    epochs=epochs,\n",
    "                    use_multiprocessing=True,\n",
    "                    workers=6,\n",
    "                    callbacks = [early]\n",
    "                    )\n",
    "    \n",
    "print('Model trained in {:.1f}min'.format((time.time()-t0)/60))"
   ],
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plot training curves\n",
    "Let's plot the training and validation accurcay and loss curves."
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def plot_training(history):\n",
    "    \"\"\" Plot training accuracy and loss curves\n",
    "    \n",
    "    Args:\n",
    "        history (dict): history dict obtained from fit function\n",
    "    \"\"\"\n",
    "    # Accuracy plot\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train','val'], loc='upper left')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.show()\n",
    "    \n",
    "    # Loss plot\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train','val'], loc='upper left')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.show()\n",
    "    \n",
    "plot_training(history)"
   ],
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluate model\n",
    "Now we evaluate the model with the validation and data and output the classification report and plot a confusion matrix. After analyzing the results with the validation data, we could make decisions about our model. Only at the end we will evaluate our classifer with the test set."
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "def evaluate_model(model, eval_gen):\n",
    "    \"\"\" Evaluate given model and print results.\n",
    "    Show validation loss and accuracy, classification report and \n",
    "    confusion matrix.\n",
    "\n",
    "    Args:\n",
    "        model (model): model to evaluate\n",
    "        eval_gen (ImageDataGenerator): evaluation generator\n",
    "    \"\"\"\n",
    "    # Evaluate the model\n",
    "    eval_gen.reset()\n",
    "    score = model.evaluate(eval_gen, verbose=0)\n",
    "    print('\\nLoss:', score[0])\n",
    "    print('Accuracy:', score[1])\n",
    "    \n",
    "    # Confusion Matrix (validation subset)\n",
    "    eval_gen.reset()\n",
    "    pred = model.predict(eval_gen, verbose=0)\n",
    "\n",
    "    # Assign most probable label\n",
    "    predicted_class_indices = np.argmax(pred,axis=1)\n",
    "\n",
    "    # Get class labels\n",
    "    labels = (eval_gen.class_indices)\n",
    "    target_names = labels.keys()\n",
    "\n",
    "    # Plot statistics\n",
    "    print(classification_report(eval_gen.classes, predicted_class_indices, target_names=target_names))\n",
    "\n",
    "    cf_matrix = confusion_matrix(np.array(eval_gen.classes), predicted_class_indices)\n",
    "    fig, ax = plt.subplots(figsize=(13, 13)) \n",
    "    sns.heatmap(cf_matrix, annot=True, cmap='PuRd', cbar=False, square=True, xticklabels=target_names, yticklabels=target_names)\n",
    "    plt.show()\n",
    "    \n",
    "evaluate_model(model, validation_generator)"
   ],
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test the model\n",
    "Finally, we evaluate the model on the test set."
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "evaluate_model(model, test_generator)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}